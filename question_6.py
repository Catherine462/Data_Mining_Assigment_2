# -*- coding: utf-8 -*-
"""Question_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y3tAHOHHRrWQjjcO_qRtixbU_sD3srXz
"""

from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
import numpy as np
import pandas as pd
data_test = pd.read_csv('cheat_test.csv', sep=',', header=0)
data_training = np.array(data_test)
print("this is lenf",len(data_training))
X=[]
Y=[]
for i, data in enumerate(data_training):
        features_test = []

        if data[1] == 'Yes':
            features_test.append(1)
        else:
            features_test.append(0)

        if data[2] == 'Single':
            features_test.append(1)
            features_test.extend([0, 0])
        elif data[2] == 'Divorced':
            features_test.extend([0, 1, 0])
        else:
            features_test.extend([0, 0, 1])
        taxable_income_test = float(data[3][:-1])
        features_test.append(taxable_income_test)
        print(features_test)

        X.append(features_test)
        if data[4] == 'Yes':
            Y.append(1)
        else:
            Y.append(0)
print("THIS IS Y",Y)

trainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.3)
ns_probs = np.zeros(len(testy))
# fit a decision tree model by using entropy with max depth = 2
clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)
clf = clf.fit(trainX, trainy)
# predict probabilities for all test samples (scores)
dt_probs = clf.predict_proba(testX)
print(dt_probs)
positive_probs = dt_probs[:, 1]
ns_auc = roc_auc_score(testy, ns_probs)
dt_auc = roc_auc_score(testy, dt_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Decision Tree: ROC AUC=%.3f' % (dt_auc))

# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)
dt_fpr, dt_tpr, _ = roc_curve(testy, dt_probs)

# plot the roc curve for the model
pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
pyplot.plot(dt_fpr, dt_tpr, marker='.', label='Decision Tree')

# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')

# show the legend
pyplot.legend()

# show the plot
pyplot.show()